# ü§ñ DevOpsAI - RAG Local com PDFs

**DevOpsAI** √© uma Prova de Conceito (POC) de uma solu√ß√£o de Intelig√™ncia Artificial generativa que roda 100% localmente. Ela utiliza a t√©cnica **RAG (Retrieval-Augmented Generation)** para ler documenta√ß√µes t√©cnicas em PDF e responder perguntas baseadas estritamente no conte√∫do desses arquivos.

## üöÄ Stack Tecnol√≥gica

Esta solu√ß√£o foi desenhada para ser modular e conteinerizada:

* **LLM Engine:** [Ollama](https://ollama.com/) (executando Llama 3.2, DeepSeek, Mistral ou outros modelos locais).
* **Backend:** Python + FastAPI + LangChain (Orquestra√ß√£o e Ingest√£o).
* **Frontend:** Streamlit (Interface de Chat).
* **Vector Store:** ChromaDB (Persist√™ncia de vetores e metadados).
* **Infraestrutura:** Docker Compose.

---

## üìÇ Estrutura do Projeto

```text
devopsai/
‚îú‚îÄ‚îÄ docker-compose.yml      # Orquestra√ß√£o dos containers e volumes
‚îú‚îÄ‚îÄ README.md               # Documenta√ß√£o
‚îú‚îÄ‚îÄ backend/                # API FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ main.py             # L√≥gica RAG e Endpoints
‚îî‚îÄ‚îÄ frontend/               # Interface Streamlit
    ‚îú‚îÄ‚îÄ Dockerfile
    ‚îú‚îÄ‚îÄ requirements.txt
    ‚îî‚îÄ‚îÄ app.py              # UI de Chat e Upload

```

---

## üõ†Ô∏è Como Rodar

### Pr√©-requisitos

* Docker e Docker Compose instalados.

### 1. Iniciar os Servi√ßos

Na raiz do projeto, execute o comando para construir as imagens e subir os containers:

```bash
docker compose up --build -d

```

Isso iniciar√° tr√™s servi√ßos:

* `devopsai_ollama` (Porta 11434)
* `devopsai_backend` (Porta 8000)
* `devopsai_frontend` (Porta 8501)

### 2. Baixar o Modelo de IA (Apenas na 1¬™ vez)

O container do Ollama inicia vazio. Precisamos baixar o modelo de linguagem. Execute o comando abaixo no seu terminal:

**Op√ß√£o 1: Llama 3.2 (Padr√£o)**
```bash
docker exec -it devopsai_ollama ollama run llama3.2
```

*Isso far√° o download do modelo Llama 3.2 (~2GB). Quando terminar e aparecer um prompt `>>>`, voc√™ pode digitar `/bye` ou pressionar `Ctrl+D` para sair.*

**Op√ß√£o 2: DeepSeek R1 (Recomendado para velocidade)**

Para usar o DeepSeek (modelo r√°pido e eficiente), execute:

```bash
docker exec -it devopsai_ollama ollama pull deepseek-r1:1.5b
```

Ou para melhor qualidade (requer mais RAM):
```bash
docker exec -it devopsai_ollama ollama pull deepseek-r1:8b
```

Depois, atualize a vari√°vel `MODEL_NAME` em `backend/main.py` para `"deepseek-r1:1.5b"` ou `"deepseek-r1:8b"` e reinicie o backend:
```bash
docker compose restart backend
```

> **Nota:** Se desejar um modelo mais robusto e tiver hardware suficiente (8GB+ RAM), voc√™ pode usar `llama3`, `mistral` ou `deepseek-r1:8b`. Sempre atualize a vari√°vel `MODEL_NAME` em `backend/main.py` ao trocar de modelo.

---

## üìñ Como Usar

1. Acesse o frontend no navegador: **[http://localhost:8501](https://www.google.com/search?q=http://localhost:8501)**.
2. Na barra lateral (**Sidebar**):
* Clique em **"Browse files"** e selecione um PDF t√©cnico.
* Clique em **"Processar PDF"**. Aguarde a mensagem de sucesso.
* Clique em **"üîÑ Atualizar Lista"** para ver seu documento indexado na Base de Conhecimento.


3. No chat principal:
* Fa√ßa uma pergunta espec√≠fica sobre o conte√∫do do PDF.
* A IA ir√° processar e responder com base no contexto encontrado.



---

## üöÄ Usando DeepSeek (Modelo R√°pido)

O **DeepSeek** √© uma excelente alternativa ao Llama 3.2, oferecendo respostas mais r√°pidas mantendo boa qualidade. Est√° dispon√≠vel localmente via Ollama.

### Op√ß√µes de Modelos DeepSeek:

- **`deepseek-r1:1.5b`** (~1.1GB) - Mais r√°pido, ideal para respostas r√°pidas
- **`deepseek-r1:8b`** (~4.7GB) - Melhor qualidade, requer mais RAM
- **`deepseek`** - Vers√£o padr√£o do DeepSeek Chat
- **`deepseek-coder`** - Especializado em c√≥digo e programa√ß√£o

### Passos para habilitar DeepSeek:

1. **Baixar o modelo:**
   ```bash
   docker exec -it devopsai_ollama ollama pull deepseek-r1:1.5b
   ```

2. **Atualizar o backend:**
   Edite o arquivo `backend/main.py` e altere a linha:
   ```python
   MODEL_NAME = "deepseek-r1:1.5b"
   ```

3. **Reiniciar o backend:**
   ```bash
   docker compose restart backend
   ```

### Gerenciar Modelos Instalados:

**Listar modelos instalados:**
```bash
docker exec devopsai_ollama ollama list
```

**Deletar um modelo antigo (para liberar espa√ßo):**
```bash
docker exec devopsai_ollama ollama rm llama3.2:latest
```

**Testar um modelo antes de usar:**
```bash
docker exec -it devopsai_ollama ollama run deepseek-r1:1.5b
```

---

## üíæ Persist√™ncia de Dados

O projeto utiliza **Docker Volumes** para garantir que os dados n√£o sejam perdidos ao reiniciar os containers:

* **`ollama_storage`**: Mant√©m os modelos baixados (Llama, Mistral, etc) em `/root/.ollama`.
* **`chroma_storage`**: Mant√©m o √≠ndice vetorial dos seus PDFs em `/app/chroma_db`.

Para reiniciar a aplica√ß√£o mantendo os dados:

```bash
docker compose restart

```

Para **apagar tudo** (resetar a IA e os documentos):

```bash
docker compose down -v

```

---

## üîå API Endpoints (Backend)

Se quiser interagir diretamente com a API (via Postman ou Curl):

* **`POST /upload`**: Envie um arquivo `multipart/form-data` (campo `file`) para indexa√ß√£o.
* **`GET /documents`**: Retorna uma lista JSON com os nomes dos arquivos j√° indexados.
* **`POST /chat`**: Envie um JSON `{"question": "Sua pergunta"}` para receber a resposta.

---

## ‚ö†Ô∏è Troubleshooting

**Erro: "Nenhum documento indexado"**

* Certifique-se de que fez o upload e clicou em "Processar PDF". Verifique a lista na sidebar.

**Erro de conex√£o com Ollama**

* Verifique se o container `devopsai_ollama` est√° rodando (`docker ps`).
* Verifique se voc√™ executou o passo 2 (download do modelo).

**Lentid√£o na resposta**

* Como √© uma IA local, a velocidade depende 100% da sua CPU/GPU. Modelos menores como `deepseek-r1:1.5b` ou `llama3.2` s√£o otimizados para velocidade. Para melhor performance, considere usar `deepseek-r1:1.5b` ou `phi3:mini`. Textos muito longos podem demorar alguns segundos mesmo assim.