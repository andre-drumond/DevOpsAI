# ü§ñ DevOpsAI - RAG Local com PDFs

**DevOpsAI** √© uma Prova de Conceito (POC) de uma solu√ß√£o de Intelig√™ncia Artificial generativa que roda 100% localmente. Ela utiliza a t√©cnica **RAG (Retrieval-Augmented Generation)** para ler documenta√ß√µes t√©cnicas em PDF e responder perguntas baseadas estritamente no conte√∫do desses arquivos.

## üöÄ Stack Tecnol√≥gica

Esta solu√ß√£o foi desenhada para ser modular e conteinerizada:

* **LLM Engine:** [Ollama](https://ollama.com/) (executando Llama 3.2 ou Mistral).
* **Backend:** Python + FastAPI + LangChain (Orquestra√ß√£o e Ingest√£o).
* **Frontend:** Streamlit (Interface de Chat).
* **Vector Store:** ChromaDB (Persist√™ncia de vetores e metadados).
* **Infraestrutura:** Docker Compose.

---

## üìÇ Estrutura do Projeto

```text
devopsai/
‚îú‚îÄ‚îÄ docker-compose.yml      # Orquestra√ß√£o dos containers e volumes
‚îú‚îÄ‚îÄ README.md               # Documenta√ß√£o
‚îú‚îÄ‚îÄ backend/                # API FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ main.py             # L√≥gica RAG e Endpoints
‚îî‚îÄ‚îÄ frontend/               # Interface Streamlit
    ‚îú‚îÄ‚îÄ Dockerfile
    ‚îú‚îÄ‚îÄ requirements.txt
    ‚îî‚îÄ‚îÄ app.py              # UI de Chat e Upload

```

---

## üõ†Ô∏è Como Rodar

### Pr√©-requisitos

* Docker e Docker Compose instalados.

### 1. Iniciar os Servi√ßos

Na raiz do projeto, execute o comando para construir as imagens e subir os containers:

```bash
docker compose up --build -d

```

Isso iniciar√° tr√™s servi√ßos:

* `devopsai_ollama` (Porta 11434)
* `devopsai_backend` (Porta 8000)
* `devopsai_frontend` (Porta 8501)

### 2. Baixar o Modelo de IA (Apenas na 1¬™ vez)

O container do Ollama inicia vazio. Precisamos baixar o modelo de linguagem. Execute o comando abaixo no seu terminal:

```bash
docker exec -it devopsai_ollama ollama run llama3.2

```

*Isso far√° o download do modelo Llama 3.2 (~2GB). Quando terminar e aparecer um prompt `>>>`, voc√™ pode digitar `/bye` ou pressionar `Ctrl+D` para sair.*

> **Nota:** Se desejar um modelo mais robusto e tiver hardware suficiente (8GB+ RAM), voc√™ pode substituir `llama3.2` por `llama3` ou `mistral`. Lembre-se de atualizar a vari√°vel `MODEL_NAME` em `backend/main.py`.

---

## üìñ Como Usar

1. Acesse o frontend no navegador: **[http://localhost:8501](https://www.google.com/search?q=http://localhost:8501)**.
2. Na barra lateral (**Sidebar**):
* Clique em **"Browse files"** e selecione um PDF t√©cnico.
* Clique em **"Processar PDF"**. Aguarde a mensagem de sucesso.
* Clique em **"üîÑ Atualizar Lista"** para ver seu documento indexado na Base de Conhecimento.


3. No chat principal:
* Fa√ßa uma pergunta espec√≠fica sobre o conte√∫do do PDF.
* A IA ir√° processar e responder com base no contexto encontrado.



---

## üíæ Persist√™ncia de Dados

O projeto utiliza **Docker Volumes** para garantir que os dados n√£o sejam perdidos ao reiniciar os containers:

* **`ollama_storage`**: Mant√©m os modelos baixados (Llama, Mistral, etc) em `/root/.ollama`.
* **`chroma_storage`**: Mant√©m o √≠ndice vetorial dos seus PDFs em `/app/chroma_db`.

Para reiniciar a aplica√ß√£o mantendo os dados:

```bash
docker compose restart

```

Para **apagar tudo** (resetar a IA e os documentos):

```bash
docker compose down -v

```

---

## üîå API Endpoints (Backend)

Se quiser interagir diretamente com a API (via Postman ou Curl):

* **`POST /upload`**: Envie um arquivo `multipart/form-data` (campo `file`) para indexa√ß√£o.
* **`GET /documents`**: Retorna uma lista JSON com os nomes dos arquivos j√° indexados.
* **`POST /chat`**: Envie um JSON `{"question": "Sua pergunta"}` para receber a resposta.

---

## ‚ö†Ô∏è Troubleshooting

**Erro: "Nenhum documento indexado"**

* Certifique-se de que fez o upload e clicou em "Processar PDF". Verifique a lista na sidebar.

**Erro de conex√£o com Ollama**

* Verifique se o container `devopsai_ollama` est√° rodando (`docker ps`).
* Verifique se voc√™ executou o passo 2 (download do modelo).

**Lentid√£o na resposta**

* Como √© uma IA local, a velocidade depende 100% da sua CPU/GPU. O modelo `llama3.2` √© otimizado para velocidade, mas textos muito longos podem demorar alguns segundos.