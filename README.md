# ü§ñ DevOpsAI - RAG Local com M√∫ltiplos Formatos

**DevOpsAI** √© uma Prova de Conceito (POC) de uma solu√ß√£o de Intelig√™ncia Artificial generativa que roda 100% localmente. Ela utiliza a t√©cnica **RAG (Retrieval-Augmented Generation)** para ler documenta√ß√µes t√©cnicas em m√∫ltiplos formatos (PDF, TXT, Markdown, DOCX) e responder perguntas baseadas estritamente no conte√∫do desses arquivos.

## üöÄ Stack Tecnol√≥gica

Esta solu√ß√£o foi desenhada para ser modular e conteinerizada:

* **LLM Engine:** [Ollama](https://ollama.com/) (executando Llama 3.2, DeepSeek, Mistral ou outros modelos locais).
* **Backend:** Python + FastAPI + LangChain (Orquestra√ß√£o e Ingest√£o).
* **Frontend:** Streamlit (Interface de Chat).
* **Vector Store:** ChromaDB (Persist√™ncia de vetores e metadados).
* **Infraestrutura:** Docker Compose.

---

## üìÇ Estrutura do Projeto

```text
devopsai/
‚îú‚îÄ‚îÄ docker-compose.yml      # Orquestra√ß√£o dos containers e volumes
‚îú‚îÄ‚îÄ README.md               # Documenta√ß√£o
‚îú‚îÄ‚îÄ backend/                # API FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ main.py             # L√≥gica RAG e Endpoints
‚îî‚îÄ‚îÄ frontend/               # Interface Streamlit
    ‚îú‚îÄ‚îÄ Dockerfile
    ‚îú‚îÄ‚îÄ requirements.txt
    ‚îî‚îÄ‚îÄ app.py              # UI de Chat e Upload

```

---

## üõ†Ô∏è Como Rodar

### Pr√©-requisitos

* Docker e Docker Compose instalados.

### 1. Iniciar os Servi√ßos

Na raiz do projeto, execute o comando para construir as imagens e subir os containers:

```bash
docker compose up --build -d

```

Isso iniciar√° tr√™s servi√ßos:

* `devopsai_ollama` (Porta 11434)
* `devopsai_backend` (Porta 8000)
* `devopsai_frontend` (Porta 8501)

### 2. Baixar o Modelo de IA (Apenas na 1¬™ vez)

O container do Ollama inicia vazio. Voc√™ pode baixar modelos de duas formas:

**Op√ß√£o 1: Pela Interface Web (Recomendado) üéâ**

1. Acesse o frontend em [http://localhost:8501](http://localhost:8501)
2. Na barra lateral, expanda a se√ß√£o **"ü§ñ Modelo de IA"** e depois **"üì¶ Gerenciador de Modelos"**
3. V√° para a aba **"üì• Dispon√≠veis"**
4. Use a busca para encontrar o modelo desejado (ex: `deepseek-r1:1.5b`)
5. Clique em **"Baixar"** e acompanhe o progresso (percentual em texto)
6. Aguarde a conclus√£o do download

**Op√ß√£o 2: Via Terminal (Alternativa)**

Se preferir usar o terminal, execute:

```bash
# Baixar Llama 3.2
docker exec -it devopsai_ollama ollama pull llama3.2

# Ou baixar DeepSeek R1 (recomendado para velocidade)
docker exec -it devopsai_ollama ollama pull deepseek-r1:1.5b

# Para melhor qualidade (requer mais RAM)
docker exec -it devopsai_ollama ollama pull deepseek-r1:8b
```

> **Nota:** Com as novas funcionalidades, voc√™ pode escolher o modelo diretamente na interface do frontend, sem precisar editar c√≥digo! Basta selecionar o modelo desejado na barra lateral.

## ü§ñ Gerenciamento de Modelos

A aplica√ß√£o permite gerenciar modelos de IA diretamente pela interface web!

### Funcionalidades

1. **Ver Modelos Instalados**: Lista todos os modelos j√° baixados no Ollama
2. **Baixar Novos Modelos**: Download de modelos diretamente pelo frontend com progresso em tempo real (percentual)
3. **Remover Modelos**: Delete modelos que n√£o s√£o mais necess√°rios para liberar espa√ßo
4. **Sele√ß√£o de Modelo**: Escolha qual modelo usar para as conversas

### Como Usar

1. **Acesse a se√ß√£o "ü§ñ Modelo de IA"** na barra lateral (expandida por padr√£o)
2. **Selecione um modelo**: Use o dropdown "Escolha o modelo" (mostra apenas modelos instalados)
3. **Gerenciar modelos**: Expanda a subse√ß√£o **"üì¶ Gerenciador de Modelos"**
   - **Aba "üì• Dispon√≠veis"**: Veja e baixe novos modelos
     - Use a busca para encontrar o modelo desejado
     - Clique em "Baixar" ao lado do modelo
     - Acompanhe o progresso do download (percentual em texto)
     - Aguarde a conclus√£o (pode levar alguns minutos dependendo do tamanho)
   - **Aba "‚úÖ Instalados"**: Veja e gerencie modelos j√° baixados
     - Lista todos os modelos instalados com seus tamanhos
     - Clique no bot√£o üóëÔ∏è para remover um modelo

### Modelos Dispon√≠veis

A aplica√ß√£o suporta diversos modelos populares:

- **DeepSeek R1**: Modelos r√°pidos e eficientes (1.5B e 8B)
- **Llama**: Modelos da Meta (Llama 3.2, Llama 3, Llama 3.1)
- **Mistral**: Modelo de alta qualidade
- **Phi-3**: Modelo leve da Microsoft
- **Gemma 2**: Modelo do Google
- **Qwen 2.5**: Modelo de alta qualidade

### API Endpoints para Modelos

```bash
# Listar modelos instalados
GET /models/installed

# Listar modelos dispon√≠veis
GET /models/available

# Listar tudo (instalados + dispon√≠veis)
GET /models

# Baixar um modelo (streaming)
POST /models/pull
{
  "model": "deepseek-r1:1.5b"
}

# Remover um modelo
DELETE /models/{model_name}

# Informa√ß√µes sobre um modelo
GET /models/{model_name}/info
```

---

## üìñ Como Usar

### Estrutura da Interface

A interface est√° organizada em duas √°reas principais:

- **Barra Lateral (Esquerda)**: Configura√ß√µes e gerenciamento
  - **ü§ñ Modelo de IA**: Sele√ß√£o de modelo e gerenciador de modelos
  - **üéõÔ∏è Par√¢metros de Resposta**: Configura√ß√µes de temperatura, top_p e streaming
  - **üìÇ Gerenciar Documentos**: Upload e gerenciamento de documentos
  
- **√Årea Principal (Direita)**: Chat com a IA
  - Hist√≥rico de conversas
  - Campo de input para perguntas
  - Respostas da IA baseadas na documenta√ß√£o

1. Acesse o frontend no navegador: **[http://localhost:8501](http://localhost:8501)**.

2. **Configurar Modelo e Par√¢metros** (na barra lateral):
   * **Se√ß√£o "ü§ñ Modelo de IA"** (expandida por padr√£o):
     - Escolha o modelo de IA no dropdown (mostra apenas modelos instalados)
     - Se n√£o houver modelos instalados, baixe um na subse√ß√£o "üì¶ Gerenciador de Modelos"
   * **Se√ß√£o "üéõÔ∏è Par√¢metros de Resposta"**:
     - Ajuste a **Temperatura** (0.0-2.0): controla criatividade
     - Ajuste o **Top P** (0.0-1.0): controla diversidade
     - Ative **Streaming de Respostas** para respostas em tempo real

3. **Upload de Documentos** (na barra lateral - se√ß√£o "üìÇ Gerenciar Documentos"):
   * Clique em **"Browse files"** e selecione um arquivo (PDF, TXT, MD, DOCX)
   * Opcionalmente, expanda "‚öôÔ∏è Metadados (Opcional)" e adicione metadados customizados em JSON
   * Clique em **"üì§ Processar Arquivo"** e aguarde a indexa√ß√£o
   * Clique em **"üîÑ Atualizar Lista"** para ver documentos indexados

4. **Gerenciar Documentos**:
   * Veja detalhes de cada documento expandindo o card (chunks, hash, metadados)
   * Use os bot√µes **"‚ÑπÔ∏è Detalhes"** e **"üóëÔ∏è Deletar"** para gerenciar documentos

5. **Chat com a IA** (√°rea principal):
   * Fa√ßa perguntas sobre o conte√∫do dos documentos no campo de chat
   * As respostas s√£o baseadas exclusivamente na base de conhecimento
   * Com streaming ativado, veja a resposta sendo gerada em tempo real
   * Use o bot√£o **"üóëÔ∏è Limpar Conversa"** para resetar o hist√≥rico



---

### Modelos Recomendados

- **`deepseek-r1:1.5b`** (~1.1GB) - Mais r√°pido, ideal para respostas r√°pidas ‚ö°
- **`deepseek-r1:8b`** (~4.7GB) - Melhor qualidade, requer mais RAM üéØ
- **`llama3.2`** (~2GB) - Modelo balanceado da Meta ‚öñÔ∏è
- **`mistral`** (~4.1GB) - Alta qualidade üåü

**Nota:** Tudo pode ser feito pela interface web! N√£o √© mais necess√°rio editar c√≥digo ou usar comandos do terminal para gerenciar modelos.

---

## üíæ Persist√™ncia de Dados

O projeto utiliza **Docker Volumes** para garantir que os dados n√£o sejam perdidos ao reiniciar os containers:

* **`ollama_storage`**: Mant√©m os modelos baixados (Llama, Mistral, etc) em `/root/.ollama`.
* **`chroma_storage`**: Mant√©m o √≠ndice vetorial dos seus PDFs em `/app/chroma_db`.

Para reiniciar a aplica√ß√£o mantendo os dados:

```bash
docker compose restart

```

Para **apagar tudo** (resetar a IA e os documentos):

```bash
docker compose down -v

```

---

## üîå API Endpoints (Backend)

### Endpoints Principais

* **`GET /health`**: Healthcheck do backend
* **`GET /models`**: Lista modelos dispon√≠veis
* **`POST /upload`**: Upload de arquivos (PDF, TXT, MD, DOCX)
  ```bash
  curl -X POST "http://localhost:8000/upload" \
    -F "file=@documento.pdf" \
    -F "custom_metadata={\"categoria\":\"devops\"}"
  ```

* **`GET /documents`**: Lista todos os documentos indexados com metadados
* **`GET /documents/{filename}`**: Detalhes de um documento espec√≠fico
* **`DELETE /documents/{filename}`**: Remove um documento da base
* **`PUT /documents/{filename}/metadata`**: Atualiza metadados de um documento
  ```bash
  curl -X PUT "http://localhost:8000/documents/arquivo.pdf/metadata" \
    -H "Content-Type: application/json" \
    -d '{"custom_metadata": {"versao": "2.0"}}'
  ```

* **`POST /chat`**: Chat com a IA (suporta streaming)
  ```bash
  # Modo normal
  curl -X POST "http://localhost:8000/chat" \
    -H "Content-Type: application/json" \
    -d '{
      "question": "O que √© Docker?",
      "model": "deepseek-r1:1.5b",
      "temperature": 0.7,
      "top_p": 0.9,
      "stream": false
    }'
  
  # Modo streaming (SSE)
  curl -X POST "http://localhost:8000/chat" \
    -H "Content-Type: application/json" \
    -d '{"question": "Explique RAG", "stream": true}' \
    --no-buffer
  ```

## ‚ú® Funcionalidades Avan√ßadas

### üìÑ Suporte a M√∫ltiplos Formatos

A aplica√ß√£o agora suporta:
- **PDF** (.pdf) - Documentos PDF padr√£o
- **Texto** (.txt) - Arquivos de texto simples
- **Markdown** (.md, .markdown) - Documenta√ß√£o em Markdown
- **Word** (.docx) - Documentos Microsoft Word

### üóÇÔ∏è Gerenciamento de Documentos

- **Listar documentos**: Veja todos os documentos indexados com informa√ß√µes detalhadas
- **Detalhes**: Visualize chunks, hash do arquivo e metadados
- **Deletar**: Remova documentos da base de conhecimento
- **Metadados customizados**: Adicione informa√ß√µes extras aos documentos (categoria, vers√£o, tags, etc.)

### ü§ñ M√∫ltiplos Modelos e Configura√ß√µes

- **Sele√ß√£o de modelo**: Escolha entre diferentes modelos (DeepSeek, Llama, Mistral, etc.) diretamente na sidebar
- **Download de modelos**: Baixe modelos diretamente pela interface com progresso em tempo real (percentual em texto)
- **Gerenciamento de modelos**: Visualize modelos instalados, baixe novos e remova modelos desnecess√°rios
- **Temperatura**: Ajuste a criatividade das respostas (0.0 = determin√≠stico, 2.0 = criativo)
- **Top P**: Controle a diversidade das respostas (0.0-1.0)
- **Configura√ß√£o por requisi√ß√£o**: Cada pergunta pode usar configura√ß√µes diferentes

### ‚ö° Streaming de Respostas

- **Respostas em tempo real**: Veja a resposta sendo gerada token por token
- **Melhor UX**: Feedback imediato ao usu√°rio
- **SSE (Server-Sent Events)**: Implementa√ß√£o eficiente de streaming

### üîí Valida√ß√µes e Seguran√ßa

- **Valida√ß√£o de formato**: Apenas formatos suportados s√£o aceitos
- **Limite de tamanho**: Arquivos limitados a 50MB
- **Detec√ß√£o de duplicatas**: Hash MD5 para identificar arquivos duplicados
- **Valida√ß√£o de metadados**: JSON validado antes de processar

---

## ‚ö†Ô∏è Troubleshooting

**Erro: "Nenhum documento indexado"**

* Certifique-se de que fez o upload e clicou em "üì§ Processar Arquivo". Verifique a lista na se√ß√£o "üìÇ Gerenciar Documentos" na sidebar.

**Erro de conex√£o com Ollama**

* Verifique se o container `devopsai_ollama` est√° rodando (`docker ps`).
* Verifique se voc√™ executou o passo 2 (download do modelo).

**Lentid√£o na resposta**

* Como √© uma IA local, a velocidade depende 100% da sua CPU/GPU. Modelos menores como `deepseek-r1:1.5b` ou `llama3.2` s√£o otimizados para velocidade. Para melhor performance, considere usar `deepseek-r1:1.5b` ou `phi3:mini`. Textos muito longos podem demorar alguns segundos mesmo assim.